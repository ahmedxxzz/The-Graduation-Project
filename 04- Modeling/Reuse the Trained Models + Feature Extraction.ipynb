{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import logging\n",
    "logging.getLogger('absl').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_image(model_path, image_path):\n",
    "    # Extract model name from the filename (e.g., 'VGG16_custom_head_model.h5' â†’ 'VGG16')\n",
    "    model_name = os.path.basename(model_path).split('_')[0]\n",
    "    \n",
    "    # Set image size based on model\n",
    "    img_h, img_w = (299, 299) if model_name == 'Xception' else (224, 224)\n",
    "    \n",
    "    # Map model name to preprocessing function and base model class\n",
    "    model_config = {\n",
    "        'VGG19': (tf.keras.applications.vgg19.preprocess_input, tf.keras.applications.VGG19),\n",
    "        'ResNet50': (tf.keras.applications.resnet50.preprocess_input, tf.keras.applications.ResNet50),\n",
    "        'VGG16': (tf.keras.applications.vgg16.preprocess_input, tf.keras.applications.VGG16),\n",
    "        'MobileNetV2': (tf.keras.applications.mobilenet_v2.preprocess_input, tf.keras.applications.MobileNetV2),\n",
    "        'Xception': (tf.keras.applications.xception.preprocess_input, tf.keras.applications.Xception),\n",
    "        'EfficientNetB0': (tf.keras.applications.efficientnet.preprocess_input, tf.keras.applications.EfficientNetB0),\n",
    "        'DenseNet121': (tf.keras.applications.densenet.preprocess_input, tf.keras.applications.DenseNet121)\n",
    "    }\n",
    "    \n",
    "    preprocess_input, base_model_class = model_config[model_name]\n",
    "    \n",
    "    # Create feature extractor (base model + pooling)\n",
    "    base_model = base_model_class(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\n",
    "    feature_extractor = tf.keras.Model(\n",
    "        inputs=base_model.input,\n",
    "        outputs=tf.keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    )\n",
    "    \n",
    "    # Load custom head model\n",
    "    custom_head_model = tf.keras.models.load_model(model_path)\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    img = image.load_img(image_path, target_size=(img_h, img_w))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_preprocessed = preprocess_input(img_array)\n",
    "    # Extract features and predict\n",
    "    features = feature_extractor.predict(img_preprocessed,verbose=0)\n",
    "    prediction = custom_head_model.predict(features, verbose=0)\n",
    "    \n",
    "    imag_name =  os.path.basename(image_path)\n",
    "    # Convert prediction to class label (assuming class 0: 'oblique', class 1: 'overriding')\n",
    "    return (model_name, imag_name,'overriding' if prediction[0][0] > 0.5 else 'oblique')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000024654C17420> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000024654C17420> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000024654C154E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000024654C154E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('DenseNet121', 'o1.png', 'oblique'), ('EfficientNetB0', 'o1.png', 'oblique'), ('MobileNetV2', 'o1.png', 'oblique'), ('ResNet50', 'o1.png', 'oblique'), ('VGG16', 'o1.png', 'oblique'), ('VGG19', 'o1.png', 'oblique'), ('Xception', 'o1.png', 'oblique'), ('DenseNet121', 'o2.png', 'oblique'), ('EfficientNetB0', 'o2.png', 'oblique'), ('MobileNetV2', 'o2.png', 'oblique'), ('ResNet50', 'o2.png', 'oblique'), ('VGG16', 'o2.png', 'oblique'), ('VGG19', 'o2.png', 'oblique'), ('Xception', 'o2.png', 'oblique'), ('DenseNet121', 'v1.png', 'overriding'), ('EfficientNetB0', 'v1.png', 'overriding'), ('MobileNetV2', 'v1.png', 'oblique'), ('ResNet50', 'v1.png', 'overriding'), ('VGG16', 'v1.png', 'overriding'), ('VGG19', 'v1.png', 'oblique'), ('Xception', 'v1.png', 'overriding'), ('DenseNet121', 'v2.png', 'overriding'), ('EfficientNetB0', 'v2.png', 'overriding'), ('MobileNetV2', 'v2.png', 'overriding'), ('ResNet50', 'v2.png', 'overriding'), ('VGG16', 'v2.png', 'overriding'), ('VGG19', 'v2.png', 'overriding'), ('Xception', 'v2.png', 'overriding')]\n",
      "the prediction result of the \u001b[91mDenseNet121\u001b[0m model for this image o1.png is \u001b[92m oblique\u001b[0m class \n",
      "the prediction result of the \u001b[91mEfficientNetB0\u001b[0m model for this image o1.png is \u001b[92m oblique\u001b[0m class \n",
      "the prediction result of the \u001b[91mMobileNetV2\u001b[0m model for this image o1.png is \u001b[92m oblique\u001b[0m class \n",
      "the prediction result of the \u001b[91mResNet50\u001b[0m model for this image o1.png is \u001b[92m oblique\u001b[0m class \n",
      "the prediction result of the \u001b[91mVGG16\u001b[0m model for this image o1.png is \u001b[92m oblique\u001b[0m class \n",
      "the prediction result of the \u001b[91mVGG19\u001b[0m model for this image o1.png is \u001b[92m oblique\u001b[0m class \n",
      "the prediction result of the \u001b[91mXception\u001b[0m model for this image o1.png is \u001b[92m oblique\u001b[0m class \n",
      "the prediction result of the \u001b[91mDenseNet121\u001b[0m model for this image o2.png is \u001b[92m oblique\u001b[0m class \n",
      "the prediction result of the \u001b[91mEfficientNetB0\u001b[0m model for this image o2.png is \u001b[92m oblique\u001b[0m class \n",
      "the prediction result of the \u001b[91mMobileNetV2\u001b[0m model for this image o2.png is \u001b[92m oblique\u001b[0m class \n",
      "the prediction result of the \u001b[91mResNet50\u001b[0m model for this image o2.png is \u001b[92m oblique\u001b[0m class \n",
      "the prediction result of the \u001b[91mVGG16\u001b[0m model for this image o2.png is \u001b[92m oblique\u001b[0m class \n",
      "the prediction result of the \u001b[91mVGG19\u001b[0m model for this image o2.png is \u001b[92m oblique\u001b[0m class \n",
      "the prediction result of the \u001b[91mXception\u001b[0m model for this image o2.png is \u001b[92m oblique\u001b[0m class \n",
      "the prediction result of the \u001b[91mDenseNet121\u001b[0m model for this image v1.png is \u001b[92m overriding\u001b[0m class \n",
      "the prediction result of the \u001b[91mEfficientNetB0\u001b[0m model for this image v1.png is \u001b[92m overriding\u001b[0m class \n",
      "the prediction result of the \u001b[91mMobileNetV2\u001b[0m model for this image v1.png is \u001b[92m oblique\u001b[0m class \n",
      "the prediction result of the \u001b[91mResNet50\u001b[0m model for this image v1.png is \u001b[92m overriding\u001b[0m class \n",
      "the prediction result of the \u001b[91mVGG16\u001b[0m model for this image v1.png is \u001b[92m overriding\u001b[0m class \n",
      "the prediction result of the \u001b[91mVGG19\u001b[0m model for this image v1.png is \u001b[92m oblique\u001b[0m class \n",
      "the prediction result of the \u001b[91mXception\u001b[0m model for this image v1.png is \u001b[92m overriding\u001b[0m class \n",
      "the prediction result of the \u001b[91mDenseNet121\u001b[0m model for this image v2.png is \u001b[92m overriding\u001b[0m class \n",
      "the prediction result of the \u001b[91mEfficientNetB0\u001b[0m model for this image v2.png is \u001b[92m overriding\u001b[0m class \n",
      "the prediction result of the \u001b[91mMobileNetV2\u001b[0m model for this image v2.png is \u001b[92m overriding\u001b[0m class \n",
      "the prediction result of the \u001b[91mResNet50\u001b[0m model for this image v2.png is \u001b[92m overriding\u001b[0m class \n",
      "the prediction result of the \u001b[91mVGG16\u001b[0m model for this image v2.png is \u001b[92m overriding\u001b[0m class \n",
      "the prediction result of the \u001b[91mVGG19\u001b[0m model for this image v2.png is \u001b[92m overriding\u001b[0m class \n",
      "the prediction result of the \u001b[91mXception\u001b[0m model for this image v2.png is \u001b[92m overriding\u001b[0m class \n"
     ]
    }
   ],
   "source": [
    "# model_path = '../05- Saved Models/VGG16_custom_head_model.h5'\n",
    "# image_path = '../Test Data/v3.png'\n",
    "# predicted_class = predict_image(model_path, image_path)\n",
    "# print(f'Predicted class: {predicted_class}')\n",
    "Base_Folder = 'D:/Learning/University of sadat/Grade 4/Semester 2/06- Graduation Project/Coding/'\n",
    "Models_folder = f\"{Base_Folder}05- Saved Models/\"\n",
    "Images_Folder = f\"{Base_Folder}Test Data/\"\n",
    "\n",
    "\n",
    "models=[]\n",
    "\n",
    "for file in os.listdir(Models_folder) :\n",
    "    if file.lower().endswith(\"h5\"):\n",
    "        models.append(f\"{Models_folder}{file}\")\n",
    "\n",
    "Results = []\n",
    "for img  in os.listdir(Images_Folder) :\n",
    "    for model in models:\n",
    "        Results.append(predict_image(model,f\"{Images_Folder}{img}\"))\n",
    "\n",
    "print(Results)\n",
    "\n",
    "for model_name,img_name,answer  in Results:\n",
    "    print(f\"the prediction result of the \\033[91m{model_name}\\033[0m model for this image {img_name} is \\033[92m {answer}\\033[0m class \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ai_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
