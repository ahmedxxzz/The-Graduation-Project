{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split into train, validation, and test in 'D:/Learning/University of sadat/Grade 4/Semester 2/06- Graduation Project/Coding/00- The DataSet/Dataset_split_Before_Preprocessing'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "\n",
    "def split_dataset(source_dir, output_dir, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Splits each image class in the source_dir into train, validation, and test directories.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    train_dir = os.path.join(output_dir, \"train\")\n",
    "    val_dir = os.path.join(output_dir, \"validation\")\n",
    "    test_dir = os.path.join(output_dir, \"test\")\n",
    "\n",
    "    for dir_path in [train_dir, val_dir, test_dir]:\n",
    "        if not os.path.exists(dir_path):\n",
    "            os.makedirs(dir_path)\n",
    "\n",
    "    classes = [d for d in os.listdir(\n",
    "        source_dir) if os.path.isdir(os.path.join(source_dir, d))]\n",
    "    for class_name in classes:\n",
    "        class_src = os.path.join(source_dir, class_name)\n",
    "        images = [f for f in os.listdir(\n",
    "            class_src) if os.path.isfile(os.path.join(class_src, f))]\n",
    "        random.shuffle(images)\n",
    "\n",
    "        total = len(images)\n",
    "        train_end = int(total * train_ratio)\n",
    "        val_end = train_end + int(total * val_ratio)\n",
    "\n",
    "        splits = {\n",
    "            \"train\": images[:train_end],\n",
    "            \"validation\": images[train_end:val_end],\n",
    "            \"test\": images[val_end:]\n",
    "        }\n",
    "\n",
    "        for split, file_list in splits.items():\n",
    "            class_dst = os.path.join(output_dir, split, class_name)\n",
    "            if not os.path.exists(class_dst):\n",
    "                os.makedirs(class_dst)\n",
    "            for file_name in file_list:\n",
    "                src_file = os.path.join(class_src, file_name)\n",
    "                dst_file = os.path.join(class_dst, file_name)\n",
    "                shutil.copy2(src_file, dst_file)\n",
    "    print(f\"Dataset split into train, validation, and test in '{output_dir}'.\")\n",
    "\n",
    "\n",
    "# Paths to dataset directory\n",
    "Base_Folder = 'D:/Learning/University of sadat/Grade 4/Semester 2/06- Graduation Project/Coding/'\n",
    "original_dataset_dir = f'{Base_Folder}00- The DataSet/00- Dogs Femur Fracture'\n",
    "split_dataset_dir = f'{Base_Folder}00- The DataSet/Dataset_split_Before_Preprocessing'\n",
    "split_dataset(original_dataset_dir, split_dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_flops import get_flops  # For FLOPs calculation\n",
    "import shutil\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from tensorflow.keras.applications import VGG19, ResNet50, VGG16, MobileNetV2, Xception, EfficientNetB0, DenseNet121\n",
    "\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input as vgg19_preprocess\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet50_preprocess\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg16_preprocess\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input as mobilenetv2_preprocess\n",
    "from tensorflow.keras.applications.xception import preprocess_input as xception_preprocess\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as efficientnetb0_preprocess\n",
    "from tensorflow.keras.applications.densenet import preprocess_input as densenet121_preprocess\n",
    "\n",
    "from tensorflow.keras.layers import InputLayer\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, roc_curve, auc, classification_report,roc_auc_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with VGG19...\n",
      "Found 35 images belonging to 2 classes.\n",
      "Found 3 images belonging to 2 classes.\n",
      "Found 6 images belonging to 2 classes.\n",
      "class indices: ['Oblique', 'Overriding']\n",
      "Extracting features for training set...\n",
      "Extracting features for validation set...\n",
      "Extracting features for test set...\n",
      "Epoch 1/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 349ms/step - accuracy: 0.4506 - loss: 0.7846 - val_accuracy: 0.6667 - val_loss: 0.5706\n",
      "Epoch 2/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.5199 - loss: 0.7324 - val_accuracy: 0.6667 - val_loss: 0.5207\n",
      "Epoch 3/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.6568 - loss: 0.6367 - val_accuracy: 0.6667 - val_loss: 0.4913\n",
      "Epoch 4/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.6274 - loss: 0.5715 - val_accuracy: 0.6667 - val_loss: 0.4751\n",
      "Epoch 5/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.6759 - loss: 0.5189 - val_accuracy: 0.6667 - val_loss: 0.4609\n",
      "Epoch 6/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.7158 - loss: 0.5522 - val_accuracy: 0.6667 - val_loss: 0.4434\n",
      "Epoch 7/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.7937 - loss: 0.4625 - val_accuracy: 0.6667 - val_loss: 0.4172\n",
      "Epoch 8/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.7747 - loss: 0.4996 - val_accuracy: 0.6667 - val_loss: 0.3982\n",
      "Epoch 9/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.7851 - loss: 0.4710 - val_accuracy: 0.6667 - val_loss: 0.3795\n",
      "Epoch 10/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.7262 - loss: 0.4810 - val_accuracy: 0.6667 - val_loss: 0.3659\n",
      "Epoch 11/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9116 - loss: 0.3478 - val_accuracy: 0.6667 - val_loss: 0.3512\n",
      "Epoch 12/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.8926 - loss: 0.3639 - val_accuracy: 0.6667 - val_loss: 0.3353\n",
      "Epoch 13/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.8821 - loss: 0.3919 - val_accuracy: 1.0000 - val_loss: 0.3142\n",
      "Epoch 14/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8527 - loss: 0.3322 - val_accuracy: 1.0000 - val_loss: 0.3026\n",
      "Epoch 15/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9220 - loss: 0.3119 - val_accuracy: 1.0000 - val_loss: 0.2894\n",
      "Epoch 16/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.8631 - loss: 0.3290 - val_accuracy: 1.0000 - val_loss: 0.2785\n",
      "Epoch 17/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.9515 - loss: 0.2813 - val_accuracy: 1.0000 - val_loss: 0.2674\n",
      "Epoch 18/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.9411 - loss: 0.2363 - val_accuracy: 1.0000 - val_loss: 0.2602\n",
      "Epoch 19/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.9810 - loss: 0.2763 - val_accuracy: 1.0000 - val_loss: 0.2498\n",
      "Epoch 20/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9515 - loss: 0.2777 - val_accuracy: 1.0000 - val_loss: 0.2440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Learning\\University of sadat\\Grade 4\\Semester 2\\06- Graduation Project\\Ai_Env\\Lib\\site-packages\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: keras_tensor\n",
      "Received: inputs=['Tensor(shape=(1, 224, 224, 3))']\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Learning\\University of sadat\\Grade 4\\Semester 2\\06- Graduation Project\\Ai_Env\\Lib\\site-packages\\keras_flops\\flops_calculation.py:35: The name tf.RunMetadata is deprecated. Please use tf.compat.v1.RunMetadata instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Learning\\University of sadat\\Grade 4\\Semester 2\\06- Graduation Project\\Ai_Env\\Lib\\site-packages\\keras_flops\\flops_calculation.py:36: The name tf.profiler.ProfileOptionBuilder is deprecated. Please use tf.compat.v1.profiler.ProfileOptionBuilder instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Learning\\University of sadat\\Grade 4\\Semester 2\\06- Graduation Project\\Ai_Env\\Lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py:5256: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This API was designed for TensorFlow v1. See https://www.tensorflow.org/guide/migrate for instructions on how to migrate your code to TensorFlow v2.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 354ms/step - accuracy: 0.8333 - loss: 0.2692\n",
      "\n",
      "VGG19 Test Loss: 0.2692\n",
      "VGG19 Test Accuracy: 0.8333\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VGG19 Efficiency Metrics:\n",
      "Inference Speed: 0.2002 seconds/image\n",
      "FLOPs: 39038.39 MFLOPs\n",
      "Memory Consumption: 147.29 MB\n",
      "Model Size: 81.16 MB\n",
      "Energy Consumption: 30318 J\n",
      "VGG19 custom head model saved.\n",
      "\n",
      "Training with ResNet50...\n",
      "Found 35 images belonging to 2 classes.\n",
      "Found 3 images belonging to 2 classes.\n",
      "Found 6 images belonging to 2 classes.\n",
      "class indices: ['Oblique', 'Overriding']\n",
      "Extracting features for training set...\n",
      "WARNING:tensorflow:5 out of the last 8 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000019DC21E2660> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 8 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000019DC21E2660> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000019DC21E2660> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 9 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000019DC21E2660> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for validation set...\n",
      "Extracting features for test set...\n",
      "Epoch 1/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 228ms/step - accuracy: 0.4021 - loss: 1.0109 - val_accuracy: 1.0000 - val_loss: 0.4691\n",
      "Epoch 2/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.6083 - loss: 0.7853 - val_accuracy: 1.0000 - val_loss: 0.4324\n",
      "Epoch 3/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.6759 - loss: 0.6012 - val_accuracy: 1.0000 - val_loss: 0.4036\n",
      "Epoch 4/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.6863 - loss: 0.5572 - val_accuracy: 1.0000 - val_loss: 0.3827\n",
      "Epoch 5/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.8336 - loss: 0.4301 - val_accuracy: 1.0000 - val_loss: 0.3691\n",
      "Epoch 6/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.8821 - loss: 0.3231 - val_accuracy: 1.0000 - val_loss: 0.3594\n",
      "Epoch 7/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8926 - loss: 0.2960 - val_accuracy: 1.0000 - val_loss: 0.3503\n",
      "Epoch 8/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9411 - loss: 0.2340 - val_accuracy: 1.0000 - val_loss: 0.3426\n",
      "Epoch 9/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9515 - loss: 0.2458 - val_accuracy: 1.0000 - val_loss: 0.3347\n",
      "Epoch 10/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9705 - loss: 0.2100 - val_accuracy: 1.0000 - val_loss: 0.3260\n",
      "Epoch 11/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - accuracy: 0.9705 - loss: 0.1809 - val_accuracy: 1.0000 - val_loss: 0.3172\n",
      "Epoch 12/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 0.9810 - loss: 0.1864 - val_accuracy: 1.0000 - val_loss: 0.3109\n",
      "Epoch 13/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.9619 - loss: 0.1299 - val_accuracy: 1.0000 - val_loss: 0.3051\n",
      "Epoch 14/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 0.1115 - val_accuracy: 1.0000 - val_loss: 0.3005\n",
      "Epoch 15/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9810 - loss: 0.1247 - val_accuracy: 1.0000 - val_loss: 0.2981\n",
      "Epoch 16/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.9810 - loss: 0.1352 - val_accuracy: 1.0000 - val_loss: 0.2984\n",
      "Epoch 17/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.9810 - loss: 0.1279 - val_accuracy: 1.0000 - val_loss: 0.3006\n",
      "Epoch 18/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 0.0921 - val_accuracy: 1.0000 - val_loss: 0.3025\n",
      "Epoch 19/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.9619 - loss: 0.1049 - val_accuracy: 1.0000 - val_loss: 0.3017\n",
      "Epoch 20/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 1.0000 - loss: 0.0855 - val_accuracy: 1.0000 - val_loss: 0.3017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Learning\\University of sadat\\Grade 4\\Semester 2\\06- Graduation Project\\Ai_Env\\Lib\\site-packages\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: keras_tensor_30\n",
      "Received: inputs=['Tensor(shape=(1, 224, 224, 3))']\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 1.0000 - loss: 0.2070\n",
      "\n",
      "ResNet50 Test Loss: 0.2070\n",
      "ResNet50 Test Accuracy: 1.0000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ResNet50 Efficiency Metrics:\n",
      "Inference Speed: 0.3850 seconds/image\n",
      "FLOPs: 7753.30 MFLOPs\n",
      "Memory Consumption: 247.03 MB\n",
      "Model Size: 98.56 MB\n",
      "Energy Consumption: 8630 J\n",
      "ResNet50 custom head model saved.\n",
      "\n",
      "Training with VGG16...\n",
      "Found 35 images belonging to 2 classes.\n",
      "Found 3 images belonging to 2 classes.\n",
      "Found 6 images belonging to 2 classes.\n",
      "class indices: ['Oblique', 'Overriding']\n",
      "Extracting features for training set...\n",
      "Extracting features for validation set...\n",
      "Extracting features for test set...\n",
      "Epoch 1/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 221ms/step - accuracy: 0.4905 - loss: 0.8200 - val_accuracy: 1.0000 - val_loss: 0.2222\n",
      "Epoch 2/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.4696 - loss: 0.7387 - val_accuracy: 1.0000 - val_loss: 0.1550\n",
      "Epoch 3/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.5979 - loss: 0.7142 - val_accuracy: 1.0000 - val_loss: 0.1179\n",
      "Epoch 4/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.6863 - loss: 0.6021 - val_accuracy: 1.0000 - val_loss: 0.0992\n",
      "Epoch 5/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.6568 - loss: 0.5572 - val_accuracy: 1.0000 - val_loss: 0.0924\n",
      "Epoch 6/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.6759 - loss: 0.5632 - val_accuracy: 1.0000 - val_loss: 0.0850\n",
      "Epoch 7/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.6759 - loss: 0.4973 - val_accuracy: 1.0000 - val_loss: 0.0780\n",
      "Epoch 8/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.6673 - loss: 0.4744 - val_accuracy: 1.0000 - val_loss: 0.0715\n",
      "Epoch 9/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.7851 - loss: 0.4637 - val_accuracy: 1.0000 - val_loss: 0.0679\n",
      "Epoch 10/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.7054 - loss: 0.4886 - val_accuracy: 1.0000 - val_loss: 0.0662\n",
      "Epoch 11/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.7452 - loss: 0.3896 - val_accuracy: 1.0000 - val_loss: 0.0636\n",
      "Epoch 12/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.7851 - loss: 0.3912 - val_accuracy: 1.0000 - val_loss: 0.0634\n",
      "Epoch 13/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.8821 - loss: 0.3796 - val_accuracy: 1.0000 - val_loss: 0.0626\n",
      "Epoch 14/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.8336 - loss: 0.3652 - val_accuracy: 1.0000 - val_loss: 0.0618\n",
      "Epoch 15/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.8926 - loss: 0.3429 - val_accuracy: 1.0000 - val_loss: 0.0627\n",
      "Epoch 16/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.8545 - loss: 0.3528 - val_accuracy: 1.0000 - val_loss: 0.0658\n",
      "Epoch 17/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.9411 - loss: 0.3093 - val_accuracy: 1.0000 - val_loss: 0.0676\n",
      "Epoch 18/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.9705 - loss: 0.3034 - val_accuracy: 1.0000 - val_loss: 0.0692\n",
      "Epoch 19/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.8631 - loss: 0.2826 - val_accuracy: 1.0000 - val_loss: 0.0698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Learning\\University of sadat\\Grade 4\\Semester 2\\06- Graduation Project\\Ai_Env\\Lib\\site-packages\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: keras_tensor_213\n",
      "Received: inputs=['Tensor(shape=(1, 224, 224, 3))']\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.6667 - loss: 0.6426\n",
      "\n",
      "VGG16 Test Loss: 0.6426\n",
      "VGG16 Test Accuracy: 0.6667\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VGG16 Efficiency Metrics:\n",
      "Inference Speed: 0.1437 seconds/image\n",
      "FLOPs: 30713.49 MFLOPs\n",
      "Memory Consumption: 120.84 MB\n",
      "Model Size: 59.92 MB\n",
      "Energy Consumption: 11690 J\n",
      "VGG16 custom head model saved.\n",
      "\n",
      "Training with MobileNetV2...\n",
      "Found 35 images belonging to 2 classes.\n",
      "Found 3 images belonging to 2 classes.\n",
      "Found 6 images belonging to 2 classes.\n",
      "class indices: ['Oblique', 'Overriding']\n",
      "Extracting features for training set...\n",
      "Extracting features for validation set...\n",
      "Extracting features for test set...\n",
      "Epoch 1/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 221ms/step - accuracy: 0.4905 - loss: 0.8708 - val_accuracy: 0.0000e+00 - val_loss: 0.8537\n",
      "Epoch 2/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.4905 - loss: 0.6892 - val_accuracy: 0.3333 - val_loss: 0.7627\n",
      "Epoch 3/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.7851 - loss: 0.5301 - val_accuracy: 0.3333 - val_loss: 0.7022\n",
      "Epoch 4/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.7262 - loss: 0.5466 - val_accuracy: 0.3333 - val_loss: 0.6553\n",
      "Epoch 5/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.9411 - loss: 0.3706 - val_accuracy: 0.6667 - val_loss: 0.6191\n",
      "Epoch 6/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.8527 - loss: 0.3644 - val_accuracy: 0.6667 - val_loss: 0.5864\n",
      "Epoch 7/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9220 - loss: 0.2828 - val_accuracy: 0.6667 - val_loss: 0.5589\n",
      "Epoch 8/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.9220 - loss: 0.2636 - val_accuracy: 0.6667 - val_loss: 0.5351\n",
      "Epoch 9/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.9705 - loss: 0.2645 - val_accuracy: 0.6667 - val_loss: 0.5156\n",
      "Epoch 10/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.9705 - loss: 0.2149 - val_accuracy: 0.6667 - val_loss: 0.4992\n",
      "Epoch 11/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.9515 - loss: 0.1978 - val_accuracy: 0.6667 - val_loss: 0.4850\n",
      "Epoch 12/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 0.1657 - val_accuracy: 0.6667 - val_loss: 0.4725\n",
      "Epoch 13/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.9810 - loss: 0.1689 - val_accuracy: 0.6667 - val_loss: 0.4624\n",
      "Epoch 14/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.9810 - loss: 0.1490 - val_accuracy: 0.6667 - val_loss: 0.4554\n",
      "Epoch 15/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.9810 - loss: 0.1485 - val_accuracy: 0.6667 - val_loss: 0.4475\n",
      "Epoch 16/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.9619 - loss: 0.2067 - val_accuracy: 0.6667 - val_loss: 0.4425\n",
      "Epoch 17/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.9810 - loss: 0.1067 - val_accuracy: 0.6667 - val_loss: 0.4371\n",
      "Epoch 18/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.1061 - val_accuracy: 0.6667 - val_loss: 0.4323\n",
      "Epoch 19/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0920 - val_accuracy: 0.6667 - val_loss: 0.4268\n",
      "Epoch 20/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.0913 - val_accuracy: 0.6667 - val_loss: 0.4212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Learning\\University of sadat\\Grade 4\\Semester 2\\06- Graduation Project\\Ai_Env\\Lib\\site-packages\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: keras_tensor_240\n",
      "Received: inputs=['Tensor(shape=(1, 224, 224, 3))']\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.8333 - loss: 0.5378\n",
      "\n",
      "MobileNetV2 Test Loss: 0.5378\n",
      "MobileNetV2 Test Accuracy: 0.8333\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MobileNetV2 Efficiency Metrics:\n",
      "Inference Speed: 0.1926 seconds/image\n",
      "FLOPs: 614.04 MFLOPs\n",
      "Memory Consumption: 98.93 MB\n",
      "Model Size: 11.67 MB\n",
      "Energy Consumption: 2838 J\n",
      "MobileNetV2 custom head model saved.\n",
      "\n",
      "Training with Xception...\n",
      "Found 35 images belonging to 2 classes.\n",
      "Found 3 images belonging to 2 classes.\n",
      "Found 6 images belonging to 2 classes.\n",
      "class indices: ['Oblique', 'Overriding']\n",
      "Extracting features for training set...\n",
      "Extracting features for validation set...\n",
      "Extracting features for test set...\n",
      "Epoch 1/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 253ms/step - accuracy: 0.3726 - loss: 0.9406 - val_accuracy: 0.3333 - val_loss: 0.7132\n",
      "Epoch 2/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 260ms/step - accuracy: 0.4610 - loss: 0.7561 - val_accuracy: 0.3333 - val_loss: 0.7000\n",
      "Epoch 3/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 0.5598 - loss: 0.6739 - val_accuracy: 0.3333 - val_loss: 0.6891\n",
      "Epoch 4/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 110ms/step - accuracy: 0.7348 - loss: 0.5143 - val_accuracy: 0.3333 - val_loss: 0.6796\n",
      "Epoch 5/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.8250 - loss: 0.3717 - val_accuracy: 0.3333 - val_loss: 0.6718\n",
      "Epoch 6/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 118ms/step - accuracy: 0.9116 - loss: 0.3409 - val_accuracy: 0.3333 - val_loss: 0.6653\n",
      "Epoch 7/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9429 - loss: 0.2971 - val_accuracy: 0.3333 - val_loss: 0.6592\n",
      "Epoch 8/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.9705 - loss: 0.2668 - val_accuracy: 0.3333 - val_loss: 0.6528\n",
      "Epoch 9/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - accuracy: 0.9619 - loss: 0.2448 - val_accuracy: 0.3333 - val_loss: 0.6477\n",
      "Epoch 10/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9515 - loss: 0.2223 - val_accuracy: 0.3333 - val_loss: 0.6434\n",
      "Epoch 11/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9429 - loss: 0.1969 - val_accuracy: 0.3333 - val_loss: 0.6392\n",
      "Epoch 12/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step - accuracy: 1.0000 - loss: 0.1286 - val_accuracy: 0.3333 - val_loss: 0.6348\n",
      "Epoch 13/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 1.0000 - loss: 0.1191 - val_accuracy: 0.3333 - val_loss: 0.6303\n",
      "Epoch 14/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.9619 - loss: 0.1576 - val_accuracy: 0.3333 - val_loss: 0.6266\n",
      "Epoch 15/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - accuracy: 1.0000 - loss: 0.1350 - val_accuracy: 0.3333 - val_loss: 0.6224\n",
      "Epoch 16/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9619 - loss: 0.1505 - val_accuracy: 0.3333 - val_loss: 0.6212\n",
      "Epoch 17/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 1.0000 - loss: 0.1082 - val_accuracy: 0.3333 - val_loss: 0.6218\n",
      "Epoch 18/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 1.0000 - loss: 0.0823 - val_accuracy: 0.3333 - val_loss: 0.6233\n",
      "Epoch 19/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9619 - loss: 0.1281 - val_accuracy: 0.3333 - val_loss: 0.6233\n",
      "Epoch 20/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 1.0000 - loss: 0.0906 - val_accuracy: 0.3333 - val_loss: 0.6201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Learning\\University of sadat\\Grade 4\\Semester 2\\06- Graduation Project\\Ai_Env\\Lib\\site-packages\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: keras_tensor_402\n",
      "Received: inputs=['Tensor(shape=(1, 299, 299, 3))']\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.1667 - loss: 0.7690\n",
      "\n",
      "Xception Test Loss: 0.7690\n",
      "Xception Test Accuracy: 0.1667\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Xception Efficiency Metrics:\n",
      "Inference Speed: 0.3277 seconds/image\n",
      "FLOPs: 16771.72 MFLOPs\n",
      "Memory Consumption: 339.58 MB\n",
      "Model Size: 87.65 MB\n",
      "Energy Consumption: 14523 J\n",
      "Xception custom head model saved.\n",
      "\n",
      "Training with EfficientNetB0...\n",
      "Found 35 images belonging to 2 classes.\n",
      "Found 3 images belonging to 2 classes.\n",
      "Found 6 images belonging to 2 classes.\n",
      "class indices: ['Oblique', 'Overriding']\n",
      "Extracting features for training set...\n",
      "Extracting features for validation set...\n",
      "Extracting features for test set...\n",
      "Epoch 1/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 223ms/step - accuracy: 0.4021 - loss: 0.8557 - val_accuracy: 0.6667 - val_loss: 0.6264\n",
      "Epoch 2/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.7348 - loss: 0.5836 - val_accuracy: 0.6667 - val_loss: 0.5975\n",
      "Epoch 3/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.7937 - loss: 0.5241 - val_accuracy: 0.6667 - val_loss: 0.5730\n",
      "Epoch 4/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.8926 - loss: 0.4013 - val_accuracy: 0.6667 - val_loss: 0.5518\n",
      "Epoch 5/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 0.9705 - loss: 0.3032 - val_accuracy: 0.6667 - val_loss: 0.5320\n",
      "Epoch 6/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.9411 - loss: 0.2793 - val_accuracy: 0.6667 - val_loss: 0.5154\n",
      "Epoch 7/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.9324 - loss: 0.2638 - val_accuracy: 1.0000 - val_loss: 0.5012\n",
      "Epoch 8/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9705 - loss: 0.1811 - val_accuracy: 1.0000 - val_loss: 0.4904\n",
      "Epoch 9/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.9619 - loss: 0.1747 - val_accuracy: 1.0000 - val_loss: 0.4821\n",
      "Epoch 10/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - accuracy: 0.9705 - loss: 0.1530 - val_accuracy: 1.0000 - val_loss: 0.4759\n",
      "Epoch 11/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - accuracy: 0.9810 - loss: 0.1413 - val_accuracy: 1.0000 - val_loss: 0.4705\n",
      "Epoch 12/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.1184 - val_accuracy: 1.0000 - val_loss: 0.4660\n",
      "Epoch 13/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 1.0000 - loss: 0.1097 - val_accuracy: 1.0000 - val_loss: 0.4626\n",
      "Epoch 14/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 1.0000 - loss: 0.1213 - val_accuracy: 1.0000 - val_loss: 0.4582\n",
      "Epoch 15/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 1.0000 - loss: 0.0862 - val_accuracy: 1.0000 - val_loss: 0.4537\n",
      "Epoch 16/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 1.0000 - loss: 0.0777 - val_accuracy: 1.0000 - val_loss: 0.4499\n",
      "Epoch 17/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 1.0000 - loss: 0.0671 - val_accuracy: 1.0000 - val_loss: 0.4467\n",
      "Epoch 18/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.9810 - loss: 0.0916 - val_accuracy: 1.0000 - val_loss: 0.4441\n",
      "Epoch 19/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 1.0000 - loss: 0.0672 - val_accuracy: 1.0000 - val_loss: 0.4428\n",
      "Epoch 20/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 1.0000 - loss: 0.0777 - val_accuracy: 1.0000 - val_loss: 0.4403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Learning\\University of sadat\\Grade 4\\Semester 2\\06- Graduation Project\\Ai_Env\\Lib\\site-packages\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: keras_tensor_542\n",
      "Received: inputs=['Tensor(shape=(1, 224, 224, 3))']\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.5000 - loss: 0.6309\n",
      "\n",
      "EfficientNetB0 Test Loss: 0.6309\n",
      "EfficientNetB0 Test Accuracy: 0.5000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EfficientNetB0 Efficiency Metrics:\n",
      "Inference Speed: 0.3212 seconds/image\n",
      "FLOPs: 802.10 MFLOPs\n",
      "Memory Consumption: 119.89 MB\n",
      "Model Size: 18.83 MB\n",
      "Energy Consumption: 4228 J\n",
      "EfficientNetB0 custom head model saved.\n",
      "\n",
      "Training with DenseNet121...\n",
      "Found 35 images belonging to 2 classes.\n",
      "Found 3 images belonging to 2 classes.\n",
      "Found 6 images belonging to 2 classes.\n",
      "class indices: ['Oblique', 'Overriding']\n",
      "Extracting features for training set...\n",
      "Extracting features for validation set...\n",
      "Extracting features for test set...\n",
      "Epoch 1/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 232ms/step - accuracy: 0.3622 - loss: 1.0500 - val_accuracy: 0.6667 - val_loss: 0.6058\n",
      "Epoch 2/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.4905 - loss: 0.7762 - val_accuracy: 0.6667 - val_loss: 0.5665\n",
      "Epoch 3/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.5095 - loss: 0.7556 - val_accuracy: 1.0000 - val_loss: 0.5372\n",
      "Epoch 4/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.5789 - loss: 0.7575 - val_accuracy: 1.0000 - val_loss: 0.5155\n",
      "Epoch 5/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.5875 - loss: 0.7021 - val_accuracy: 1.0000 - val_loss: 0.4988\n",
      "Epoch 6/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.4801 - loss: 0.7315 - val_accuracy: 1.0000 - val_loss: 0.4856\n",
      "Epoch 7/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.6863 - loss: 0.5861 - val_accuracy: 1.0000 - val_loss: 0.4727\n",
      "Epoch 8/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.5979 - loss: 0.5841 - val_accuracy: 1.0000 - val_loss: 0.4577\n",
      "Epoch 9/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.7158 - loss: 0.5153 - val_accuracy: 1.0000 - val_loss: 0.4435\n",
      "Epoch 10/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.7557 - loss: 0.4466 - val_accuracy: 1.0000 - val_loss: 0.4321\n",
      "Epoch 11/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.7158 - loss: 0.4505 - val_accuracy: 1.0000 - val_loss: 0.4206\n",
      "Epoch 12/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - accuracy: 0.8440 - loss: 0.4159 - val_accuracy: 1.0000 - val_loss: 0.4081\n",
      "Epoch 13/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step - accuracy: 0.7955 - loss: 0.4332 - val_accuracy: 1.0000 - val_loss: 0.3966\n",
      "Epoch 14/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.7366 - loss: 0.4295 - val_accuracy: 1.0000 - val_loss: 0.3855\n",
      "Epoch 15/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.8042 - loss: 0.3876 - val_accuracy: 1.0000 - val_loss: 0.3763\n",
      "Epoch 16/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.8527 - loss: 0.3674 - val_accuracy: 1.0000 - val_loss: 0.3695\n",
      "Epoch 17/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 96ms/step - accuracy: 0.9134 - loss: 0.2691 - val_accuracy: 1.0000 - val_loss: 0.3650\n",
      "Epoch 18/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.8336 - loss: 0.3443 - val_accuracy: 1.0000 - val_loss: 0.3597\n",
      "Epoch 19/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9134 - loss: 0.2807 - val_accuracy: 1.0000 - val_loss: 0.3538\n",
      "Epoch 20/20\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step - accuracy: 0.9220 - loss: 0.2572 - val_accuracy: 1.0000 - val_loss: 0.3480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Learning\\University of sadat\\Grade 4\\Semester 2\\06- Graduation Project\\Ai_Env\\Lib\\site-packages\\keras\\src\\models\\functional.py:238: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: keras_tensor_788\n",
      "Received: inputs=['Tensor(shape=(1, 224, 224, 3))']\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.6667 - loss: 0.6381\n",
      "\n",
      "DenseNet121 Test Loss: 0.6381\n",
      "DenseNet121 Test Accuracy: 0.6667\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DenseNet121 Efficiency Metrics:\n",
      "Inference Speed: 0.5848 seconds/image\n",
      "FLOPs: 5701.47 MFLOPs\n",
      "Memory Consumption: 229.96 MB\n",
      "Model Size: 30.26 MB\n",
      "Energy Consumption: 7262 J\n",
      "DenseNet121 custom head model saved.\n",
      "Training histories saved for each model.\n",
      "All outputs saved to directory: D:/Learning/University of sadat/Grade 4/Semester 2/06- Graduation Project/Coding/runs_codes\\Before_preprocess_2025-03-22_17-17-58\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def extract_features(generator, model, steps):\n",
    "    \"\"\"\n",
    "    Extract features from a dataset using a feature extractor model.\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _ in range(steps):\n",
    "        batch_x, batch_y = next(generator)\n",
    "        batch_features = model.predict(batch_x, verbose=0)\n",
    "        features.append(batch_features)\n",
    "        labels.append(batch_y)\n",
    "    features = np.vstack(features)[:generator.samples]  \n",
    "    labels = np.hstack(labels)[:generator.samples]\n",
    "    return features, labels\n",
    "\n",
    "def train_and_evaluate_model(model_name, preprocess_function):\n",
    "    print(f\"\\nTraining with {model_name}...\")\n",
    "    # Set image size based on model\n",
    "    if model_name == 'Xception':\n",
    "        img_h, img_w = 299, 299\n",
    "    else:\n",
    "        img_h, img_w = 224, 224\n",
    "        \n",
    "    # Create ImageDataGenerators\n",
    "    train_datagen = ImageDataGenerator(preprocessing_function=preprocess_function)\n",
    "    val_datagen = ImageDataGenerator(preprocessing_function=preprocess_function)\n",
    "    test_datagen = ImageDataGenerator(preprocessing_function=preprocess_function)\n",
    "    \n",
    "    train_dir = os.path.join(split_dataset_dir, \"train\")\n",
    "    val_dir = os.path.join(split_dataset_dir, \"validation\")\n",
    "    test_dir = os.path.join(split_dataset_dir, \"test\")\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_directory(train_dir, target_size=(img_h, img_w), batch_size=batch_size, class_mode='binary', shuffle=True, seed=42)\n",
    "    val_generator = val_datagen.flow_from_directory(val_dir, target_size=(img_h, img_w), batch_size=batch_size, class_mode='binary', shuffle=False, seed=42)\n",
    "    test_generator = test_datagen.flow_from_directory(test_dir, target_size=(img_h, img_w), batch_size=batch_size, class_mode='binary', shuffle=False, seed=42)\n",
    "    \n",
    "    class_indices = test_generator.class_indices    \n",
    "    print(f\"class indices: {list(class_indices.keys())}\")    \n",
    "    \n",
    "    # Load the pre-trained model\n",
    "    if model_name == 'VGG19':\n",
    "        Pretrained_model = VGG19(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\n",
    "    elif model_name == 'ResNet50':\n",
    "        Pretrained_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\n",
    "    elif model_name == 'VGG16':\n",
    "        Pretrained_model = VGG16(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\n",
    "    elif model_name == 'MobileNetV2':\n",
    "        Pretrained_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\n",
    "    elif model_name == 'Xception':\n",
    "        Pretrained_model = Xception(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\n",
    "    elif model_name == 'EfficientNetB0':\n",
    "        Pretrained_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\n",
    "    elif model_name == 'DenseNet121':\n",
    "        Pretrained_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(img_h, img_w, 3))\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name\")\n",
    "        \n",
    "    # Freeze the pre-trained layers\n",
    "    Pretrained_model.trainable = False\n",
    "    \n",
    "    # Create feature extractor\n",
    "    feature_extractor = Model(inputs=Pretrained_model.input, outputs=GlobalAveragePooling2D()(Pretrained_model.output))\n",
    "    \n",
    "    # Calculate steps for feature extraction\n",
    "    train_steps = math.ceil(train_generator.samples / batch_size)\n",
    "    val_steps = math.ceil(val_generator.samples / batch_size)\n",
    "    test_steps = math.ceil(test_generator.samples / batch_size)\n",
    "    \n",
    "    # Extract features\n",
    "    print(\"Extracting features for training set...\")\n",
    "    train_features, train_labels = extract_features(train_generator, feature_extractor, train_steps)\n",
    "    print(\"Extracting features for validation set...\")\n",
    "    val_features, val_labels = extract_features(val_generator, feature_extractor, val_steps)\n",
    "    print(\"Extracting features for test set...\")\n",
    "    test_features, test_labels = extract_features(test_generator, feature_extractor, test_steps)\n",
    "    \n",
    "    # Define simplified custom head\n",
    "    feature_dim = feature_extractor.output_shape[-1]\n",
    "    input_layer = Input(shape=(feature_dim,))\n",
    "    x = Dense(512)(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    predictions = Dense(1, activation='sigmoid', dtype='float32')(x)\n",
    "    custom_model = Model(inputs=input_layer, outputs=predictions)\n",
    "    \n",
    "    # Compile the custom head model\n",
    "    custom_model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    # Train the custom head\n",
    "    history = custom_model.fit(train_features, train_labels, batch_size=batch_size, epochs=epochs, \n",
    "                              validation_data=(val_features, val_labels), callbacks=[early_stopping])\n",
    "    \n",
    "    # Create full model for efficiency analysis\n",
    "    full_model = Model(inputs=feature_extractor.input, outputs=custom_model(feature_extractor.output))\n",
    "    # Efficiency Analysis\n",
    "    test_generator.reset()\n",
    "    batch_x, batch_y = next(test_generator)\n",
    "    batch_size_efficiency = len(batch_x)\n",
    "    \n",
    "    # Inference Speed\n",
    "    start_time = time.time()\n",
    "    _ = full_model.predict(batch_x, verbose=0)\n",
    "    inference_time = time.time() - start_time\n",
    "    inference_speed = inference_time / batch_size_efficiency\n",
    "    \n",
    "    # FLOPs\n",
    "    try:\n",
    "        \n",
    "        flops = get_flops(full_model)\n",
    "        flops_million = flops / 1e6\n",
    "    except Exception as eer :\n",
    "        flops_million = \"N/A\"\n",
    "        print(f\"\\n\\nFLOPs Error : {eer}\\n\\n\")\n",
    "    \n",
    "    # Inside the efficiency analysis section of train_and_evaluate_model:\n",
    "\n",
    "    # Memory Consumption (parameters + activations)\n",
    "    param_count = full_model.count_params()\n",
    "    param_memory = param_count * 4  # bytes (float32)\n",
    "    \n",
    "    activation_memory = 0\n",
    "    for layer in full_model.layers:\n",
    "        # Handle InputLayer separately\n",
    "        if isinstance(layer, InputLayer):\n",
    "            output_shape = full_model.input_shape  # Get from model's input shape\n",
    "        else:\n",
    "            # Use Keras backend to get output shape safely\n",
    "            output_shape = K.int_shape(layer.output)\n",
    "        \n",
    "        # Handle multi-output layers\n",
    "        if isinstance(output_shape, list):\n",
    "            output_shape = output_shape[0]\n",
    "        \n",
    "        # Process dimensions (skip batch dim and handle dynamic shapes)\n",
    "        activation_dims = [dim for dim in output_shape[1:] if dim is not None]\n",
    "        \n",
    "        # Calculate activation size\n",
    "        layer_activation_size = 1\n",
    "        for dim in activation_dims:\n",
    "            layer_activation_size *= dim\n",
    "        \n",
    "        activation_memory += layer_activation_size\n",
    "    \n",
    "    activation_memory *= 4  # bytes (float32)\n",
    "    total_memory = param_memory + activation_memory  # bytes\n",
    "    \n",
    "\n",
    "    # Model Size\n",
    "    model_size = param_memory\n",
    "    \n",
    "    # Energy Consumption\n",
    "    energy_consumption = \"N/A\"\n",
    "    try:\n",
    "        \n",
    "        from pyJoules.handler.csv_handler import CSVHandler\n",
    "        from pyJoules.energy_meter import measure_energy\n",
    "\n",
    "        energy_csv = os.path.join(run_dir, f\"{model_name}_energy.csv\")\n",
    "        csv_handler = CSVHandler(energy_csv)\n",
    "    \n",
    "        @measure_energy(handler=csv_handler)\n",
    "        def run_inference(model, data):\n",
    "            return model.predict(data, verbose=0)\n",
    "    \n",
    "        # Execute the decorated function\n",
    "        run_inference(full_model, batch_x)\n",
    "    \n",
    "        # Save the recorded data\n",
    "        csv_handler.save_data()\n",
    "        # Read and sum the energy consumption\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv(energy_csv,sep=';')\n",
    "        energy_consumption = df['nvidia_gpu_0'].sum()\n",
    "    except Exception as e:\n",
    "        print(f\"Energy consumption error: {e}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_accuracy = custom_model.evaluate(test_features, test_labels, verbose=1)\n",
    "    print(f\"\\n{model_name} Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"{model_name} Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = custom_model.predict(test_features, verbose=1)\n",
    "    y_pred = (predictions > 0.5).astype(int).flatten()\n",
    "    y_true = test_labels\n",
    "    \n",
    "    # Compute performance metrics\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='binary')\n",
    "    precision = precision_score(y_true, y_pred, average='binary')\n",
    "    recall = recall_score(y_true, y_pred, average='binary')\n",
    "    \n",
    "    # Compute ROC curve and AUC\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, predictions)\n",
    "    roc_auc = roc_auc_score(y_true, predictions)\n",
    "    \n",
    "    return {\n",
    "        'model': custom_model,\n",
    "        'history': history.history,\n",
    "        'confusion_matrix': cm,\n",
    "        'roc': (fpr, tpr, roc_auc) if 'fpr' in locals() else (None, None, None),\n",
    "        'performance': {'accuracy': test_accuracy, 'f1': f1, 'precision': precision, 'recall': recall},\n",
    "        'labels': list(class_indices.keys()),\n",
    "        'efficiency': {\n",
    "            'inference_speed': inference_speed,\n",
    "            'flops_million': flops_million,\n",
    "            'memory_consumption_bytes': total_memory,\n",
    "            'model_size_bytes': model_size,\n",
    "            'energy_consumption_joules': energy_consumption\n",
    "        }\n",
    "    }\n",
    "\n",
    "def plot_results(results, run_dir):\n",
    "    model_names = list(results.keys())\n",
    "    num_models = len(model_names)\n",
    "    \n",
    "    # 1. Plot Confusion Matrices\n",
    "    fig_cm, axes_cm = plt.subplots(1, num_models, figsize=(5*num_models, 4))\n",
    "    if num_models == 1:\n",
    "        axes_cm = [axes_cm]\n",
    "    for ax, name in zip(axes_cm, model_names):\n",
    "        cm = results[name]['confusion_matrix']\n",
    "        im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        ax.set_title(f\"{name} Confusion Matrix\")\n",
    "        tick_marks = np.arange(len(results[name]['labels']))\n",
    "        ax.set_xticks(tick_marks)\n",
    "        ax.set_xticklabels(results[name]['labels'], rotation=45)\n",
    "        ax.set_yticks(tick_marks)\n",
    "        ax.set_yticklabels(results[name]['labels'])\n",
    "        thresh = cm.max() / 2.0\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        fig_cm.colorbar(im, ax=ax)\n",
    "    fig_cm.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    fig_cm.suptitle(\"Confusion Matrices\", fontsize=16)\n",
    "    plt.savefig(os.path.join(run_dir, \"confusion_matrices.png\"))\n",
    "    plt.close(fig_cm)\n",
    "    \n",
    "    # 2. Plot ROC Curves\n",
    "    fig_roc, axes_roc = plt.subplots(1, num_models, figsize=(5*num_models, 4))\n",
    "    if num_models == 1:\n",
    "        axes_roc = [axes_roc]\n",
    "    for ax, name in zip(axes_roc, model_names):\n",
    "        # Get ROC data with error handling\n",
    "        roc_data = results[name].get('roc', (None, None, None))\n",
    "        fpr, tpr, roc_auc = roc_data\n",
    "        \n",
    "        # Handle missing or invalid ROC data\n",
    "        if fpr is None or tpr is None or roc_auc is None:\n",
    "            print(f\"Warning: Missing ROC data for {name}\")\n",
    "            ax.text(0.5, 0.5, 'No valid ROC data', ha='center', va='center')\n",
    "            ax.set_title(f\"ROC Curve - {name}\")\n",
    "            continue\n",
    "        \n",
    "        # Plot with formatted AUC\n",
    "        label = f\"AUC = {roc_auc:.2f}\" if isinstance(roc_auc, (int, float)) else \"AUC = N/A\"\n",
    "        ax.plot(fpr, tpr, lw=2, label=label)\n",
    "        ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        ax.set_xlim([0.0, 1.0])\n",
    "        ax.set_ylim([0.0, 1.05])\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title(f\"ROC Curve - {name}\")\n",
    "        ax.legend(loc=\"lower right\")\n",
    "    fig_roc.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    fig_roc.suptitle(\"ROC Curves\", fontsize=16)\n",
    "    plt.savefig(os.path.join(run_dir, \"roc_curves.png\"))\n",
    "    plt.close(fig_roc)\n",
    "    \n",
    "    # 3. Plot Accuracy and Loss Curves for each model\n",
    "    for name in model_names:\n",
    "        history = results[name]['history']\n",
    "        epochs_range = range(1, len(history['accuracy']) + 1)\n",
    "        fig_model, (ax_acc, ax_loss) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        ax_acc.plot(epochs_range, history['accuracy'], marker='o', label='Train Accuracy')\n",
    "        ax_acc.plot(epochs_range, history['val_accuracy'], marker='x', linestyle='--', label='Validation Accuracy')\n",
    "        ax_acc.set_title(f\"{name} Accuracy\")\n",
    "        ax_acc.set_xlabel('Epoch')\n",
    "        ax_acc.set_ylabel('Accuracy')\n",
    "        ax_acc.legend()\n",
    "        \n",
    "        ax_loss.plot(epochs_range, history['loss'], marker='o', label='Train Loss')\n",
    "        ax_loss.plot(epochs_range, history['val_loss'], marker='x', linestyle='--', label='Validation Loss')\n",
    "        ax_loss.set_title(f\"{name} Loss\")\n",
    "        ax_loss.set_xlabel('Epoch')\n",
    "        ax_loss.set_ylabel('Loss')\n",
    "        ax_loss.legend()\n",
    "        \n",
    "        fig_model.suptitle(f\"Accuracy and Loss Curves - {name}\", fontsize=16)\n",
    "        fig_model.tight_layout(rect=[0, 0, 1, 0.93])\n",
    "        plt.savefig(os.path.join(run_dir, f\"{name}_training_curves.png\"))\n",
    "        plt.close(fig_model)\n",
    "    \n",
    "    # 4. Overall Performance Comparison Table\n",
    "    col_labels = [\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"]\n",
    "    cell_text = []\n",
    "    for name in model_names:\n",
    "        perf = results[name]['performance']\n",
    "        row = [name,\n",
    "               f\"{perf['accuracy']:.4f}\",\n",
    "               f\"{perf['precision']:.4f}\",\n",
    "               f\"{perf['recall']:.4f}\",\n",
    "               f\"{perf['f1']:.4f}\"]\n",
    "        cell_text.append(row)\n",
    "    \n",
    "    fig_table, ax_table = plt.subplots(figsize=(8, len(model_names)*0.8+1))\n",
    "    ax_table.axis('tight')\n",
    "    ax_table.axis('off')\n",
    "    table = ax_table.table(cellText=cell_text, colLabels=col_labels, loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(12)\n",
    "    table.scale(1, 2)\n",
    "    fig_table.suptitle(\"Overall Performance Comparison\", fontsize=16)\n",
    "    plt.savefig(os.path.join(run_dir, \"performance_table.png\"))\n",
    "    plt.close(fig_table)\n",
    "    \n",
    "    # 5. Save training histories to pickle files\n",
    "    for name in model_names:\n",
    "        history = results[name]['history']\n",
    "        with open(os.path.join(run_dir, f\"{name}_history.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(history, f)\n",
    "    \n",
    "    # 6. Efficiency Comparison Table\n",
    "    col_labels_eff = [\"Model\", \"Inference Speed (s/img)\", \"FLOPs (MFLOPs)\", \"Memory (MB)\", \"Model Size (MB)\", \"Energy (J)\"]\n",
    "    cell_text_eff = []\n",
    "    for name in model_names:\n",
    "        eff = results[name]['efficiency']\n",
    "        row = [\n",
    "            name,\n",
    "            f\"{eff['inference_speed']:.4f}\",\n",
    "            f\"{eff['flops_million']:.2f}\" if eff['flops_million'] != 'N/A' else 'N/A',\n",
    "            f\"{eff['memory_consumption_bytes']/1e6:.2f}\",\n",
    "            f\"{eff['model_size_bytes']/1e6:.2f}\",\n",
    "            f\"{eff['energy_consumption_joules']:.2f}\" if isinstance(eff['energy_consumption_joules'], float) else eff['energy_consumption_joules']\n",
    "        ]\n",
    "        cell_text_eff.append(row)\n",
    "    \n",
    "    fig_table_eff, ax_table_eff = plt.subplots(figsize=(12, len(model_names)*0.8+1))\n",
    "    ax_table_eff.axis('tight')\n",
    "    ax_table_eff.axis('off')\n",
    "    table_eff = ax_table_eff.table(cellText=cell_text_eff, colLabels=col_labels_eff, loc='center')\n",
    "    table_eff.auto_set_font_size(False)\n",
    "    table_eff.set_fontsize(12)\n",
    "    table_eff.scale(1, 2)\n",
    "    fig_table_eff.suptitle(\"Efficiency Comparison\", fontsize=16)\n",
    "    plt.savefig(os.path.join(run_dir, \"efficiency_table.png\"))\n",
    "    plt.close(fig_table_eff)\n",
    "\n",
    "    \n",
    "    print(\"Training histories saved for each model.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Update the main execution block to use the new function as is\n",
    "if __name__ == \"__main__\":\n",
    "    Base_Folder = 'D:/Learning/University of sadat/Grade 4/Semester 2/06- Graduation Project/Coding/' \n",
    "    split_dataset_dir = f'{Base_Folder}00- The DataSet/Dataset_split_Before_Preprocessing'\n",
    "    \n",
    "    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    run_dir = os.path.join(f'{Base_Folder}runs_codes', f\"{current_time}_Before_preprocess\")\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    batch_size = 32\n",
    "    epochs = 20\n",
    "    \n",
    "    results = {}\n",
    "    models = {\n",
    "        'VGG19': vgg19_preprocess,\n",
    "        'ResNet50': resnet50_preprocess,\n",
    "        'VGG16': vgg16_preprocess,\n",
    "        'MobileNetV2': mobilenetv2_preprocess,\n",
    "        'Xception': xception_preprocess,\n",
    "        'EfficientNetB0': efficientnetb0_preprocess,\n",
    "        'DenseNet121': densenet121_preprocess\n",
    "    }\n",
    "    \n",
    "    for model_name, preprocess in models.items():\n",
    "        results[model_name] = train_and_evaluate_model(model_name, preprocess)\n",
    "\n",
    "        # Print efficiency metrics\n",
    "        eff = results[model_name]['efficiency']\n",
    "        print(f\"\\n{model_name} Efficiency Metrics:\")\n",
    "        print(f\"Inference Speed: {eff['inference_speed']:.4f} seconds/image\")\n",
    "        print(f\"FLOPs: {eff['flops_million']:.2f} MFLOPs\" \n",
    "              if isinstance(eff['flops_million'], (int, float)) \n",
    "              else f\"FLOPs: {eff['flops_million']} MFLOPs\")\n",
    "        print(f\"Memory Consumption: {eff['memory_consumption_bytes']/1e6:.2f} MB\")\n",
    "        print(f\"Model Size: {eff['model_size_bytes']/1e6:.2f} MB\")\n",
    "        print(f\"Energy Consumption: {eff['energy_consumption_joules']:.2f} J\" if isinstance(eff['energy_consumption_joules'], (int, float)) else f\"Energy Consumption: {eff['energy_consumption_joules']} J\")\n",
    "        # Save the custom head model instead\n",
    "        results[model_name]['model'].save(os.path.join(run_dir, f\"{model_name}_custom_model.h5\"))\n",
    "        print(f\"{model_name} custom head model saved.\")\n",
    "    \n",
    "    plot_results(results, run_dir)\n",
    "    print(f\"All outputs saved to directory: {run_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ai_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
